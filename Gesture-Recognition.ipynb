{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mediapipe as mp  \n",
    "import cv2            \n",
    "import numpy as np  #  For performing mathematical operations likewise in array\n",
    "import uuid #  We will not get any overlap when we click our photos. Every photo will have a unique id.\n",
    "import os  #  uuid and os will be used when we will run the program on a visual level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Solutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils  # For drawing on images like 3d boundry boxes that we are going to detect and drag around in the image frame\n",
    "mp_hands = mp.solutions.hands  #  Detect landmarks in hand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the WebCam without Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)  #  Whatever the video will capture it will be stored in this cap folder it will tell us which camera to choose \n",
    "# while cap.isOpened():\n",
    "#     ret,frame = cap.read()  #  Whatever the camera reads it will be stored in frame variable and ret i.e return variable. The frame variable  will represent the image from our webcam.\n",
    "    \n",
    "#     #  Now we are going display that image on the screen \n",
    "#     cv2.imshow('Hand Tracking',frame)  #  We had given the name of the frame as 'Hand Tracking'.\n",
    "    \n",
    "# # Below code is used to gracefully close all open windows.\n",
    "#         # Check for user input to close the window\n",
    "#     if cv2.waitKey(10) & 0xFF == ord('q'):  # Press 'Esc' key to exit\n",
    "#         break\n",
    "\n",
    "# # Release the VideoCapture object and close any open windows\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the WebCam with Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize VideoCapture to capture frames from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands solution\n",
    "with mp.solutions.hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5,max_num_hands=7) as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()  # Read frame from the webcam\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame from camera. Exiting...\")\n",
    "            break\n",
    "\n",
    "         # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  #  Here writing Frame in brackets to convert. As mediapipe aspects input in the form of rgb\n",
    "\n",
    "        image.flags.writeable = False   #  This means that you're instructing MediaPipe not to modify the image data during processing.This conversion is often done because MediaPipe typically expects input images in RGB format.\n",
    "        \n",
    "        results = hands.process(image)  #  This is the most important stage, It will process our image and return a dictionary of information about what it found\n",
    "        \n",
    "        image.flags.writeable = True #  After processing is complete, this line sets the writeable flag of the image array back to True, allowing modifications to the array.\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)#  #  Here writing Image in brackets to convert back. Now again converting the image from RGB to BGR because the mediapipe will  process in BGR format  \n",
    "        \n",
    "\n",
    "        # Display the image with hand landmarks\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS)  #  HAND_CONNECTIONS which joint is connected to which joint \n",
    "        \n",
    "        print(results)\n",
    "\n",
    "        cv2.imshow('Hand Tracking', image)  # Display the processed image\n",
    "\n",
    "        # Check for user input to close the window\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "# Release the VideoCapture object and close any open windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results.multi_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
